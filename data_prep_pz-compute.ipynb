{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee477dc5-578b-41ab-9ffb-01b43742f4ca",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; align-items: flex-start; gap: 40px;\">\n",
    "  <div style=\"display: flex; align-items: center; gap: 10px;\">\n",
    "    <img src=\"https://www.linea.org.br/brand/linea-logo-color.svg\" width=\"100\" style=\"display: block;\">\n",
    "    <img src=\"https://cdn2.webdamdb.com/1280_c3PXjCZbPM23.png\" width=\"180\" style=\"display: block;\">\n",
    "  </div>\n",
    "  \n",
    "  <div style=\"margin-left: 20px;\">\n",
    "    <h2 style=\"margin: 0 0 10px 0; padding: 0;\">Data Preparation on Object Tables<br> for PZ Compute pipeline</h2>\n",
    "    Notebook for LIneA Open OnDemand Platform<br>\n",
    "    Data Release: <a href=\"https://dp0-2.lsst.io/\">Data Preview 0.2</a> <br>\n",
    "    Authors: <a href=\"mailto:luigi.lcsilva@gmail.com\">Luigi Lucas de Carvalho Silva</a>, <a href=\"mailto:julia@linea.org.br\">Julia Gschwend</a> <br>\n",
    "    Last verified to run: 2025-08-16 <br>\n",
    "    Repository: <a href=\"https://github.com/linea-it/pz-lsst-inkind\">linea-it/pz-lsst-inkind</a> <br>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6198345-745a-4d9b-8f71-a6bcde9d4aa9",
   "metadata": {},
   "source": [
    "**tl;dr:**\n",
    "\n",
    "This notebook prepares the photometric input catalog for the PZ Compute pipeline using LSST Object Catalog data. The process includes:\n",
    "- Convert fluxes to magnitudes\n",
    "- Apply dereddening corrections\n",
    "- Round numerical values to eliminate excessive precision\n",
    "- Mask invalid entries\n",
    "- Apply quality cuts\n",
    "- Save balanced parquet files with ~N rows (N chosen by user)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd66a097-157a-4ad2-8cc0-7bc1e90709ea",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8524989f-b700-4c66-a3aa-fcf0b4313d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################### GENERAL ##########################################\n",
    "import os\n",
    "import gc\n",
    "import glob\n",
    "import time\n",
    "import math\n",
    "import yaml\n",
    "import shutil\n",
    "import logging\n",
    "import tables_io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import getpass\n",
    "from datetime import datetime\n",
    "\n",
    "##################################### DASK ###########################################\n",
    "import dask\n",
    "from dask import config as dask_config_class\n",
    "from dask import dataframe as dd\n",
    "from dask import delayed, compute\n",
    "from dask.distributed import Client, performance_report, wait\n",
    "from dask_jobqueue import SLURMCluster\n",
    "\n",
    "##################################### ASTROPY ###########################################\n",
    "from astropy.coordinates import SkyCoord\n",
    "import astropy.units as u\n",
    "from astropy.io import fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8356ae-879b-4281-b8cf-4870926e7099",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "globals().update(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac79bf6-8470-4324-9626-a4cc872e5a10",
   "metadata": {},
   "source": [
    "# Setups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7382c350-6bf5-4797-ae3d-21ea97135459",
   "metadata": {},
   "source": [
    "Making the setup of dustmaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38937bcc-88ec-447e-9b5b-631d91c39f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### DUSTMAPS SETUP ###########################################\n",
    "from dustmaps.config import config as dust_config\n",
    "import importlib\n",
    "\n",
    "# Point dustmaps to your parent directory (works for all workers if they see the same FS)\n",
    "if 'DP_path_to_dustmaps' in globals() and DP_path_to_dustmaps:\n",
    "    dust_config['data_dir'] = DP_path_to_dustmaps\n",
    "    # Also set env var for good measure (some setups rely on it)\n",
    "    os.environ.setdefault('DUSTMAPS_PATH', DP_path_to_dustmaps)\n",
    "\n",
    "# Normalize model name\n",
    "DP_use_dustmap = (DP_use_dustmap or 'sfd').strip().lower()\n",
    "\n",
    "# Which models require distance? Extend as needed.\n",
    "_THREED_MODELS = {'bayestar', 'marshall', 'stilism'}\n",
    "\n",
    "# Lazy singleton for the chosen dustmap query\n",
    "_DUST_QUERY = None\n",
    "\n",
    "def _make_dust_query(name: str):\n",
    "    \"\"\"\n",
    "    Dynamically import and construct the dust map query object by name.\n",
    "    Supported examples:\n",
    "      - 'sfd'       -> dustmaps.sfd.SFDQuery (2D E(B-V))\n",
    "      - 'bayestar'  -> dustmaps.bayestar.BayestarQuery (3D E(B-V) requires distance)\n",
    "      - 'planck'    -> dustmaps.planck.PlanckQuery (returns E(B-V) proxy)\n",
    "    Extend this mapper if you use other maps.\n",
    "    \"\"\"\n",
    "    if name == 'sfd':\n",
    "        from dustmaps.sfd import SFDQuery\n",
    "        return SFDQuery()\n",
    "    if name.startswith('bayestar'):\n",
    "        # e.g. 'bayestar' or 'bayestar2019'\n",
    "        from dustmaps.bayestar import BayestarQuery\n",
    "        return BayestarQuery()  # configure kwargs if you need (e.g., version)\n",
    "    if name == 'planck':\n",
    "        from dustmaps.planck import PlanckQuery\n",
    "        return PlanckQuery()\n",
    "    # Fallback: try to import dustmaps.<name>.<ClassNamedLikeName>Query\n",
    "    try:\n",
    "        mod = importlib.import_module(f'dustmaps.{name}')\n",
    "        # Heuristic: pick first attribute ending with 'Query'\n",
    "        for attr in dir(mod):\n",
    "            if attr.lower().endswith('query'):\n",
    "                return getattr(mod, attr)()\n",
    "    except Exception:\n",
    "        pass\n",
    "    raise ValueError(f\"Unsupported DP_use_dustmap='{name}'. Known examples: 'sfd', 'bayestar', 'planck'.\")\n",
    "\n",
    "def get_dust_query():\n",
    "    \"\"\"Return a process-wide singleton of the chosen dust query (safe for Dask workers).\"\"\"\n",
    "    global _DUST_QUERY\n",
    "    if _DUST_QUERY is None:\n",
    "        _DUST_QUERY = _make_dust_query(DP_use_dustmap)\n",
    "    return _DUST_QUERY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ff955f-b181-47d9-a6b7-f7516d266960",
   "metadata": {},
   "source": [
    "Other setups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c8a79a-85d4-4169-a251-7691925d6e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAG_CONV = np.log(10) * 0.4\n",
    "\n",
    "if DP_col_value_to_replace is None:\n",
    "    DP_col_value_to_replace = np.nan\n",
    "if DP_err_value_to_replace is None:\n",
    "    DP_err_value_to_replace = np.nan\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Decide suffix and operational flags based on input_col_type and flags\n",
    "# (keeps the same validation/error logic and operational flags)\n",
    "# --------------------------------------------------------------------\n",
    "ALLOWED_INPUT_TYPES = {\"flux\", \"flux_dered\", \"mag\", \"mag_dered\"}\n",
    "\n",
    "def decide_suffix_and_flags(input_col_type: str,\n",
    "                            compute_mag: bool,\n",
    "                            compute_dered: bool):\n",
    "    \"\"\"\n",
    "    Returns (suffix, WILL_COMPUTE_MAG, WILL_DEREDDEN_ON_FLUX, WILL_DEREDDEN_ON_MAG)\n",
    "    and validates the chosen combination according to the defined rules.\n",
    "    NOTE: 'suffix' here is just a placeholder (legacy); the final richer suffix\n",
    "    will be rebuilt below.\n",
    "    \"\"\"\n",
    "\n",
    "    if input_col_type not in ALLOWED_INPUT_TYPES:\n",
    "        raise ValueError(f\"Invalid input_col_type='{input_col_type}'. \"\n",
    "                         f\"Allowed: {sorted(ALLOWED_INPUT_TYPES)}\")\n",
    "\n",
    "    compute_mag = bool(compute_mag)\n",
    "    compute_dered = bool(compute_dered)\n",
    "\n",
    "    # Default flags\n",
    "    WILL_COMPUTE_MAG = False\n",
    "    WILL_DEREDDEN_ON_FLUX = False\n",
    "    WILL_DEREDDEN_ON_MAG = False\n",
    "\n",
    "    if input_col_type == \"flux\":\n",
    "        if compute_mag and compute_dered:\n",
    "            suffix = \"_mag_dered\"\n",
    "            WILL_COMPUTE_MAG = True\n",
    "            WILL_DEREDDEN_ON_FLUX = True\n",
    "        elif compute_mag and not compute_dered:\n",
    "            suffix = \"_mag\"\n",
    "            WILL_COMPUTE_MAG = True\n",
    "        elif not compute_mag and compute_dered:\n",
    "            suffix = \"_flux_dered\"\n",
    "            WILL_DEREDDEN_ON_FLUX = True\n",
    "        else:\n",
    "            suffix = \"_flux\"\n",
    "\n",
    "    elif input_col_type == \"flux_dered\":\n",
    "        if compute_dered:\n",
    "            raise ValueError(\"Cannot deredden an already dereddened flux (input_col_type='flux_dered').\")\n",
    "        if compute_mag:\n",
    "            suffix = \"_mag_dered\"\n",
    "            WILL_COMPUTE_MAG = True\n",
    "        else:\n",
    "            suffix = \"_flux_dered\"\n",
    "\n",
    "    elif input_col_type == \"mag\":\n",
    "        if compute_mag:\n",
    "            raise ValueError(\"Cannot compute magnitude when input is already magnitude (input_col_type='mag').\")\n",
    "        if compute_dered:\n",
    "            suffix = \"_mag_dered\"\n",
    "            WILL_DEREDDEN_ON_MAG = True\n",
    "        else:\n",
    "            suffix = \"_mag\"\n",
    "\n",
    "    elif input_col_type == \"mag_dered\":\n",
    "        if compute_mag:\n",
    "            raise ValueError(\"Cannot compute magnitude when input is already magnitude (input_col_type='mag_dered').\")\n",
    "        if compute_dered:\n",
    "            raise ValueError(\"Cannot deredden an already dereddened magnitude (input_col_type='mag_dered').\")\n",
    "        suffix = \"_mag_dered\"\n",
    "\n",
    "    return suffix, WILL_COMPUTE_MAG, WILL_DEREDDEN_ON_FLUX, WILL_DEREDDEN_ON_MAG\n",
    "\n",
    "\n",
    "# Call the decision logic (keeps validations and operational flags)\n",
    "suffix, WILL_COMPUTE_MAG, WILL_DEREDDEN_ON_FLUX, WILL_DEREDDEN_ON_MAG = decide_suffix_and_flags(\n",
    "    input_col_type=input_col_type,\n",
    "    compute_mag=DP_compute_magnitude,\n",
    "    compute_dered=DP_compute_dereddening,\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Rebuild a richer 'suffix' for reuse:\n",
    "# suffix = _{DP_which_release}_{out_kind}_{input_col_model?}_{dust_tag?}\n",
    "# --------------------------------------------------------------------\n",
    "# out_kind = 'mag' if the final output is in magnitudes (or input is already mag),\n",
    "# otherwise 'flux'\n",
    "_out_kind = 'mag' if (WILL_COMPUTE_MAG or str(input_col_type).startswith('mag')) else 'flux'\n",
    "\n",
    "# dust_tag:\n",
    "# - If dereddening is applied now ‚Üí use DP_use_dustmap (lowercased)\n",
    "# - If input is already *_dered and no new dereddening ‚Üí 'dered'\n",
    "# - Otherwise ‚Üí omit\n",
    "if (WILL_DEREDDEN_ON_FLUX or WILL_DEREDDEN_ON_MAG):\n",
    "    _dust_tag = (str(DP_use_dustmap).strip().lower()\n",
    "                 if 'DP_use_dustmap' in globals() and DP_use_dustmap\n",
    "                 else 'dered')\n",
    "elif str(input_col_type).endswith('_dered'):\n",
    "    _dust_tag = 'dered'\n",
    "else:\n",
    "    _dust_tag = None\n",
    "\n",
    "# Optional model token (if not null/empty in YAML)\n",
    "if isinstance(cfg.get(\"input_col_model\"), str):\n",
    "    _model_token = cfg[\"input_col_model\"].strip() or None\n",
    "else:\n",
    "    _model_token = cfg.get(\"input_col_model\") or None\n",
    "\n",
    "# Build the richer suffix tokens\n",
    "_suffix_tokens = [str(DP_which_release), _out_kind]\n",
    "if _model_token:\n",
    "    _suffix_tokens.append(str(_model_token))\n",
    "if _dust_tag:\n",
    "    _suffix_tokens.append(_dust_tag)\n",
    "\n",
    "# Replace old 'suffix' with this new one, prefixed by underscore\n",
    "suffix = \"_\".join(_suffix_tokens)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Define execution folder name using the new suffix\n",
    "# --------------------------------------------------------------------\n",
    "current_date = datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "run_folder = f\"DP_run_{suffix}_{current_date}\"\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Output paths\n",
    "# --------------------------------------------------------------------\n",
    "os.makedirs(user_base_path, exist_ok=True)\n",
    "\n",
    "run_path = os.path.join(user_base_path, run_folder)\n",
    "os.makedirs(run_path, exist_ok=True)\n",
    "\n",
    "data_dir = os.path.join(run_path, 'data')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "logs_dir = os.path.join(run_path, 'logs')\n",
    "os.makedirs(logs_dir, exist_ok=True)\n",
    "\n",
    "dask_logs_dir = os.path.join(run_path, 'dask_logs')\n",
    "os.makedirs(dask_logs_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f98b2d-452a-4fff-9b6b-5e24331f5bcc",
   "metadata": {},
   "source": [
    "Making a copy of the .yaml in the logs_dir folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d06570-60d8-4ac4-877b-36fadec619fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.copy(\"config.yaml\", os.path.join(logs_dir, \"config.yaml\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f393d6d9-f06a-4ed8-ba55-51a8e3c4bc33",
   "metadata": {},
   "source": [
    "# Initializing the Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835401d9-d3dc-4395-a6a4-5a88f526523d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLUSTER_extra_dask_configs:\n",
    "    dask_config_class.set(CLUSTER_dask_config)\n",
    "else:\n",
    "    print(\"Running DASK with the standard memory configuration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dacb63-aa9f-4806-9fcb-85d954728bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_date = datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "\n",
    "if CLUSTER_save_the_dask_jobs_info:\n",
    "    CLUSTER_job_extra_directives=[\n",
    "        '--propagate',\n",
    "        f'--account={CLUSTER_account}',\n",
    "        f'--output={dask_logs_dir}/dask_job_%j_{current_date}.out',  \n",
    "        f'--error={dask_logs_dir}/dask_job_%j_{current_date}.err',\n",
    "    ]\n",
    "else:\n",
    "    CLUSTER_job_extra_directives=[\n",
    "        '--propagate',\n",
    "        f'--account={CLUSTER_account}',\n",
    "        f'--output=/dev/null',  \n",
    "        f'--error=/dev/null'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db964cc-7053-4209-a6d2-d1d5d93179c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring the SLURMCluster.\n",
    "cluster = SLURMCluster(\n",
    "    interface=CLUSTER_interface,         # Lustre interface\n",
    "    queue=CLUSTER_queue,                 # Name of the queue\n",
    "    cores=CLUSTER_cores,                 # Number of logical cores per node\n",
    "    processes=CLUSTER_processes,         # Number of dask processes per node\n",
    "    memory=CLUSTER_memory,               # Memory per node\n",
    "    walltime=CLUSTER_walltime,           # Maximum execution time              \n",
    "    job_extra_directives=CLUSTER_job_extra_directives,\n",
    ")\n",
    "\n",
    "# Scaling the cluster to use X nodes\n",
    "cluster.scale(jobs=CLUSTER_dask_scale_number)\n",
    "\n",
    "# Defining the dask client\n",
    "client = Client(cluster)\n",
    "\n",
    "# Wait for 90% of the workers to initialize\n",
    "cluster.wait_for_workers(n_workers=(math.ceil(CLUSTER_dask_scale_number * CLUSTER_processes * 0.9)))\n",
    "client.run(lambda: gc.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20631f19-f046-4b1d-84ae-84acd4c09755",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_info = client.cluster\n",
    "cluster_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeba9d8c-17b8-4b22-9fac-5a78c5b3e770",
   "metadata": {},
   "source": [
    "# Processing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cc24a6-4992-4cd3-82ee-b9d55a3b84d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files = [f for f in glob.glob(os.path.join(input_catalog_folder, input_catalog_pattern))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1769748b-17c2-49cb-8168-5054f3bf6cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_path = os.path.join(logs_dir, \"dask-performance-report.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4ccf5a-18f4-409b-92de-353f9a7c72c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fits_to_df_no_fix(filename, columns=None):\n",
    "    with fits.open(filename, memmap=True) as hdul:\n",
    "        data = hdul[1].data\n",
    "        df = pd.DataFrame(data)\n",
    "        if input_user_selected_cols is not None:\n",
    "            df = df[input_user_selected_cols]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9bc266-3700-463f-bd55-b87cc3000fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@delayed\n",
    "def process_file_df(path):\n",
    "    \"\"\"\n",
    "    Process one file (Parquet or FITS) and return a pandas.DataFrame.\n",
    "    This function does not save to disk; it returns a processed DataFrame\n",
    "    to be combined later into a global Dask DataFrame.\n",
    "    \"\"\"\n",
    "    # Read depending on release type\n",
    "    if DP_which_release == 'LSST_DP02':\n",
    "        df = pd.read_parquet(path, columns=input_user_selected_cols)\n",
    "    elif DP_which_release == 'DES_DR2':\n",
    "        df = read_fits_to_df_no_fix(path, columns=input_user_selected_cols)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported DP_which_release='{DP_which_release}'\")\n",
    "\n",
    "    # Reset index if IDs are in the index\n",
    "    if DP_is_id_in_the_index:\n",
    "        df = df.reset_index()\n",
    "\n",
    "    # Apply boolean column filter if requested\n",
    "    if DP_filter_by_boolean_column:\n",
    "        if DP_which_boolean_column not in df.columns:\n",
    "            raise ValueError(f\"Boolean column '{DP_which_boolean_column}' not found in DataFrame.\")\n",
    "        df = df[df[DP_which_boolean_column] == DP_which_value_to_keep]\n",
    "\n",
    "    # Cross-invalidation guardrails\n",
    "    if DP_replace_invalid_values and DP_cross_invalidate:\n",
    "        if DP_how_to_replace_col_values != \"all\" or DP_how_to_replace_err_values != \"all\":\n",
    "            raise ValueError(\"If DP_cross_invalidate=True, both DP_how_to_replace_* must be 'all'.\")\n",
    "\n",
    "    # If we will compute magnitude, ensure output pattern differs from input pattern\n",
    "    if WILL_COMPUTE_MAG and DP_col_final_name_pattern == input_col_pattern:\n",
    "        raise ValueError(\"Output column pattern must differ from input_col_pattern when computing magnitude.\")\n",
    "\n",
    "    # Compute E(B-V) only if needed (any dereddening on flux or mag)\n",
    "    needs_ebv = bool(WILL_DEREDDEN_ON_FLUX or WILL_DEREDDEN_ON_MAG)\n",
    "    if needs_ebv:\n",
    "        dq = get_dust_query()\n",
    "    \n",
    "        # Build coordinates; for 3D maps we need a distance (per-row or fixed)\n",
    "        if DP_use_dustmap in _THREED_MODELS:\n",
    "            # Choose distance from column or fixed config\n",
    "            dist_pc = None\n",
    "            if 'DP_distance_col_pc' in globals() and DP_distance_col_pc:\n",
    "                if DP_distance_col_pc not in df.columns:\n",
    "                    raise ValueError(f\"Distance column '{DP_distance_col_pc}' not found but DP_use_dustmap is 3D ('{DP_use_dustmap}').\")\n",
    "                dist_pc = df[DP_distance_col_pc].values.astype(float)\n",
    "            elif 'DP_distance_fixed_pc' in globals() and DP_distance_fixed_pc:\n",
    "                dist_pc = np.full(len(df), float(DP_distance_fixed_pc), dtype=float)\n",
    "    \n",
    "            if dist_pc is None:\n",
    "                raise ValueError(\n",
    "                    f\"DP_use_dustmap='{DP_use_dustmap}' requires a distance. \"\n",
    "                    f\"Provide DP_distance_col_pc (column in parsecs) OR DP_distance_fixed_pc (float parsecs).\"\n",
    "                )\n",
    "    \n",
    "            coords = SkyCoord(\n",
    "                ra=df[ra_col].values * u.deg,\n",
    "                dec=df[dec_col].values * u.deg,\n",
    "                distance=dist_pc * u.pc\n",
    "            )\n",
    "        else:\n",
    "            # 2D maps (e.g., SFD) need only (ra, dec)\n",
    "            coords = SkyCoord(ra=df[ra_col].values * u.deg,\n",
    "                              dec=df[dec_col].values * u.deg)\n",
    "    \n",
    "        # Query returns E(B-V) along each sightline (or proxy, depending on model)\n",
    "        df[\"E_BV\"] = dq(coords)\n",
    "\n",
    "    # Helpers for invalid-value handling\n",
    "    def get_invalid_masks(values, errors):\n",
    "        invalid_val = np.zeros(len(values), dtype=bool)\n",
    "        invalid_err = np.zeros(len(errors), dtype=bool)\n",
    "\n",
    "        if DP_set_some_limit_as_invalid_for_col:\n",
    "            invalid_val |= (np.abs(values) >= DP_invalid_limit_value_for_col)\n",
    "        if DP_set_some_limit_as_invalid_for_err:\n",
    "            invalid_err |= (np.abs(errors) >= DP_invalid_limit_value_for_err)\n",
    "        if DP_is_nan_and_inf_invalid_for_col:\n",
    "            invalid_val |= ~np.isfinite(values)\n",
    "        if DP_is_nan_and_inf_invalid_for_err:\n",
    "            invalid_err |= ~np.isfinite(errors)\n",
    "        if DP_cross_invalidate and DP_how_to_replace_col_values == \"all\" and DP_how_to_replace_err_values == \"all\":\n",
    "            invalid_err |= invalid_val\n",
    "            invalid_val |= invalid_err\n",
    "\n",
    "        return invalid_val, invalid_err\n",
    "\n",
    "    def apply_replacement_locally(arr, mask, replacement_value):\n",
    "        return np.where(mask, replacement_value, arr)\n",
    "\n",
    "    flux_cols_to_drop = []\n",
    "\n",
    "    # Loop over all bands\n",
    "    for band in selected_bands:\n",
    "        col_in = input_col_pattern.replace(\"BAND\", band)\n",
    "        err_in = input_err_pattern.replace(\"BAND\", band)\n",
    "\n",
    "        band_fmt = {\n",
    "            \"lower_case\": band.lower(),\n",
    "            \"upper_case\": band.upper()\n",
    "        }.get(DP_pesonalized_which_band_case, band)\n",
    "\n",
    "        final_col = DP_col_final_name_pattern.replace(\"BAND\", band_fmt)\n",
    "        final_err_col = DP_err_final_name_pattern.replace(\"BAND\", band_fmt)\n",
    "\n",
    "        if col_in not in df.columns or err_in not in df.columns:\n",
    "            raise ValueError(f\"Missing column(s) {[col_in, err_in]} in file {path}\")\n",
    "\n",
    "        # Start from the input columns\n",
    "        values = df[col_in].astype(float, copy=False)\n",
    "        errors = df[err_in].astype(float, copy=False)\n",
    "\n",
    "        # 1) Deredden on flux (if requested). Applies only when working in flux space.\n",
    "        if WILL_DEREDDEN_ON_FLUX:\n",
    "            # A_lambda = R_band * E(B-V)\n",
    "            A_lambda = df[\"E_BV\"] * A_EBV[band]  # mag\n",
    "            # Flux correction factor = 10^(+0.4 * A_lambda)\n",
    "            factor = np.power(10.0, 0.4 * A_lambda)\n",
    "            values = values * factor\n",
    "            errors = errors * factor\n",
    "\n",
    "        # 2) Compute magnitude (if requested). Use CURRENT flux (after any dereddening).\n",
    "        if WILL_COMPUTE_MAG:\n",
    "            # Keep a safe copy of current flux for error propagation\n",
    "            f_curr = values\n",
    "            # m = -2.5 log10(f) + MAG_OFFSET\n",
    "            values = -2.5 * np.log10(f_curr) + MAG_OFFSET\n",
    "            # œÉ_m = œÉ_f / (f * ln(10) * 0.4) = œÉ_f / (f * MAG_CONV)\n",
    "            errors = errors / (f_curr * MAG_CONV)\n",
    "\n",
    "        # 3) Deredden on magnitude (if requested). Applies only when in magnitude space.\n",
    "        if WILL_DEREDDEN_ON_MAG:\n",
    "            A_lambda = df[\"E_BV\"] * A_EBV[band]\n",
    "            values = values - A_lambda\n",
    "            # Magnitude errors are unaffected by an additive shift\n",
    "\n",
    "        # Mark input flux columns for dropping if we either computed mag\n",
    "        # or applied dereddening in flux space and the user doesn't want to keep them.\n",
    "        if (WILL_COMPUTE_MAG or WILL_DEREDDEN_ON_FLUX) and not DP_keep_flux_columns_when_computing_mag_or_dered:\n",
    "            flux_cols_to_drop.extend([col_in, err_in])\n",
    "\n",
    "        # Invalid value replacement (applies to whichever space we are in now)\n",
    "        if DP_replace_invalid_values:\n",
    "            invalid_val, invalid_err = get_invalid_masks(values, errors)\n",
    "\n",
    "            if DP_how_to_replace_col_values == \"all\":\n",
    "                values = apply_replacement_locally(values, invalid_val, DP_col_value_to_replace)\n",
    "            elif DP_how_to_replace_col_values == \"only_with_invalid_err\":\n",
    "                values = apply_replacement_locally(values, invalid_err, DP_col_value_to_replace)\n",
    "\n",
    "            if DP_how_to_replace_err_values == \"all\":\n",
    "                errors = apply_replacement_locally(errors, invalid_err, DP_err_value_to_replace)\n",
    "            elif DP_how_to_replace_err_values == \"only_with_invalid_col\":\n",
    "                errors = apply_replacement_locally(errors, invalid_val, DP_err_value_to_replace)\n",
    "\n",
    "        # Optional rounding\n",
    "        if DP_round_col:\n",
    "            values = np.round(values, DP_round_col_decimal_cases)\n",
    "        if DP_round_err:\n",
    "            errors = np.round(errors, DP_round_err_decimal_cases)\n",
    "\n",
    "        # Write final columns\n",
    "        df[final_col] = values\n",
    "        df[final_err_col] = errors\n",
    "\n",
    "    # Clean up helper column if we created it\n",
    "    if needs_ebv and \"E_BV\" in df.columns:\n",
    "        df.drop(columns=[\"E_BV\"], inplace=True)\n",
    "\n",
    "    # Drop original flux columns if requested and applicable\n",
    "    if (WILL_COMPUTE_MAG or WILL_DEREDDEN_ON_FLUX) and not DP_keep_flux_columns_when_computing_mag_or_dered:\n",
    "        df.drop(columns=[c for c in set(flux_cols_to_drop) if c in df.columns], inplace=True)\n",
    "\n",
    "    # Optionally drop the boolean filter column after filtering\n",
    "    if DP_filter_by_boolean_column and DP_drop_column_after_filter:\n",
    "        if DP_which_boolean_column in df.columns:\n",
    "            df.drop(columns=[DP_which_boolean_column], inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3312d1f5-a23f-40ce-8241-829c2c15eb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# ‚îÄ‚îÄ Build the list of delayed DataFrames (order-preserving) ‚îÄ‚îÄ\n",
    "delayed_dfs = [process_file_df(p) for p in input_files]\n",
    "\n",
    "# Combine into a single Dask DataFrame (lazy, preserves order of delayed_dfs)\n",
    "ddf = dd.from_delayed(delayed_dfs)\n",
    "\n",
    "# Normalize config flags\n",
    "has_target = ('DP_target_rows_per_part' in globals()\n",
    "              and DP_target_rows_per_part is not None\n",
    "              and int(DP_target_rows_per_part) > 0)\n",
    "\n",
    "has_order = ('DP_order_by' in globals()\n",
    "             and DP_order_by not in (None, False, '', []))\n",
    "\n",
    "# Normalize DP_order_by to a list of columns (if provided)\n",
    "order_cols = None\n",
    "if has_order:\n",
    "    if isinstance(DP_order_by, str):\n",
    "        order_cols = [DP_order_by]\n",
    "    elif isinstance(DP_order_by, (list, tuple)):\n",
    "        order_cols = list(DP_order_by)\n",
    "    else:\n",
    "        raise ValueError(\"DP_order_by must be None, str, or list/tuple of str.\")\n",
    "    # Sanity check: all order columns must exist\n",
    "    missing = [c for c in order_cols if c not in ddf.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"DP_order_by columns not found in dataframe: {missing}\")\n",
    "\n",
    "with performance_report(filename=report_path):\n",
    "\n",
    "    # CASES:\n",
    "    # 1) has_target and has_order  -> global ordering + uniform rechunk\n",
    "    # 2) has_target and not has_order -> uniform rechunk only\n",
    "    # 3) not has_target and has_order -> per-partition ordering only (no global shuffle)\n",
    "    # 4) neither -> no ordering, no rechunk\n",
    "\n",
    "    if has_target and has_order:\n",
    "        # ---- Global ordering (ascending) ----\n",
    "        # Dask supports one-column sort natively. For multi-column, chain stable sorts (reverse priority).\n",
    "        if len(order_cols) == 1:\n",
    "            ddf_sorted = ddf.sort_values(order_cols[0], shuffle=\"p2p\")\n",
    "        else:\n",
    "            ddf_sorted = ddf\n",
    "            for col in reversed(order_cols):\n",
    "                ddf_sorted = ddf_sorted.sort_values(col, shuffle=\"p2p\")\n",
    "\n",
    "        # ---- Uniform rechunk without shuffle (order-preserving) ----\n",
    "        ddf_in = ddf_sorted\n",
    "\n",
    "        # 1) lengths and totals\n",
    "        part_lengths = ddf_in.map_partitions(len).compute()\n",
    "        total_rows = int(sum(int(x) for x in part_lengths))\n",
    "\n",
    "        target_rows = int(DP_target_rows_per_part)\n",
    "        n_parts = max(1, math.ceil(total_rows / target_rows))\n",
    "\n",
    "        base = total_rows // n_parts\n",
    "        remainder = total_rows % n_parts\n",
    "        chunk_sizes = [(base + 1) if i < remainder else base for i in range(n_parts)]\n",
    "\n",
    "        delayed_src_parts = ddf_in.to_delayed()\n",
    "        meta = ddf_in._meta\n",
    "\n",
    "        @delayed\n",
    "        def _slice_pdf(pdf, start, end):\n",
    "            \"\"\"Return pdf.iloc[start:end].\"\"\"\n",
    "            return pdf.iloc[start:end]\n",
    "\n",
    "        def _concat_pieces(pieces):\n",
    "            \"\"\"Concat a list of delayed pandas DataFrames, preserving order.\"\"\"\n",
    "            return delayed(pd.concat)(pieces, ignore_index=True)\n",
    "\n",
    "        delayed_new_parts = []\n",
    "        i_part, i_off = 0, 0\n",
    "        src_n = len(delayed_src_parts)\n",
    "\n",
    "        for need in chunk_sizes:\n",
    "            remaining = need\n",
    "            pieces = []\n",
    "            while remaining > 0 and i_part < src_n:\n",
    "                part_len = int(part_lengths[i_part])\n",
    "                if i_off >= part_len:\n",
    "                    i_part += 1\n",
    "                    i_off = 0\n",
    "                    continue\n",
    "                can_take = part_len - i_off\n",
    "                take_now = min(remaining, can_take)\n",
    "                d_piece = _slice_pdf(delayed_src_parts[i_part], i_off, i_off + take_now)\n",
    "                pieces.append(d_piece)\n",
    "                i_off += take_now\n",
    "                remaining -= take_now\n",
    "                if i_off >= part_len:\n",
    "                    i_part += 1\n",
    "                    i_off = 0\n",
    "            if pieces:\n",
    "                delayed_new_parts.append(_concat_pieces(pieces))\n",
    "\n",
    "        ddf_out = dd.from_delayed(delayed_new_parts, meta=meta)\n",
    "\n",
    "    elif has_target and not has_order:\n",
    "        # ---- No ordering; uniform rechunk only ----\n",
    "        ddf_in = ddf\n",
    "\n",
    "        part_lengths = ddf_in.map_partitions(len).compute()\n",
    "        total_rows = int(sum(int(x) for x in part_lengths))\n",
    "\n",
    "        target_rows = int(DP_target_rows_per_part)\n",
    "        n_parts = max(1, math.ceil(total_rows / target_rows))\n",
    "\n",
    "        base = total_rows // n_parts\n",
    "        remainder = total_rows % n_parts\n",
    "        chunk_sizes = [(base + 1) if i < remainder else base for i in range(n_parts)]\n",
    "\n",
    "        delayed_src_parts = ddf_in.to_delayed()\n",
    "        meta = ddf_in._meta\n",
    "\n",
    "        @delayed\n",
    "        def _slice_pdf(pdf, start, end):\n",
    "            return pdf.iloc[start:end]\n",
    "\n",
    "        def _concat_pieces(pieces):\n",
    "            return delayed(pd.concat)(pieces, ignore_index=True)\n",
    "\n",
    "        delayed_new_parts = []\n",
    "        i_part, i_off = 0, 0\n",
    "        src_n = len(delayed_src_parts)\n",
    "\n",
    "        for need in chunk_sizes:\n",
    "            remaining = need\n",
    "            pieces = []\n",
    "            while remaining > 0 and i_part < src_n:\n",
    "                part_len = int(part_lengths[i_part])\n",
    "                if i_off >= part_len:\n",
    "                    i_part += 1\n",
    "                    i_off = 0\n",
    "                    continue\n",
    "                can_take = part_len - i_off\n",
    "                take_now = min(remaining, can_take)\n",
    "                d_piece = _slice_pdf(delayed_src_parts[i_part], i_off, i_off + take_now)\n",
    "                pieces.append(d_piece)\n",
    "                i_off += take_now\n",
    "                remaining -= take_now\n",
    "                if i_off >= part_len:\n",
    "                    i_part += 1\n",
    "                    i_off = 0\n",
    "            if pieces:\n",
    "                delayed_new_parts.append(_concat_pieces(pieces))\n",
    "\n",
    "        ddf_out = dd.from_delayed(delayed_new_parts, meta=meta)\n",
    "\n",
    "    elif (not has_target) and has_order:\n",
    "        # ---- Per-partition ordering only (no global shuffle) ----\n",
    "        # Uses pandas' sort_values inside each partition; preserves the original partitioning.\n",
    "        def _sort_partition(pdf, cols):\n",
    "            return pdf.sort_values(cols, ascending=True)\n",
    "\n",
    "        ddf_out = ddf.map_partitions(_sort_partition, order_cols, meta=ddf._meta)\n",
    "\n",
    "    else:\n",
    "        # ---- No ordering, no rechunk ----\n",
    "        ddf_out = ddf\n",
    "\n",
    "    # ‚îÄ‚îÄ Saving (Parquet/CSV/HDF5 via tables_io per partition) ‚îÄ‚îÄ\n",
    "    ext = {\"parquet\": \"parquet\", \"csv\": \"csv\", \"hdf5\": \"h5\"}[save_output_as]\n",
    "\n",
    "    # Build a file-naming function consistent across formats\n",
    "    def _file_name(i: int) -> str:\n",
    "        return f\"{suffix}_part{i}.{ext}\"\n",
    "\n",
    "    if save_output_as == \"parquet\":\n",
    "        # Dask lets you customize the part filenames via name_function\n",
    "        ddf_out.to_parquet(\n",
    "            data_dir,\n",
    "            write_index=False,\n",
    "            name_function=_file_name  # Dask will call with i = 0..nparts-1\n",
    "        )\n",
    "\n",
    "    elif save_output_as == \"csv\":\n",
    "        # Same idea for CSV; no need for a glob pattern when name_function is provided\n",
    "        ddf_out.to_csv(\n",
    "            data_dir,\n",
    "            index=False,\n",
    "            name_function=_file_name\n",
    "        )\n",
    "\n",
    "    elif save_output_as == \"hdf5\":\n",
    "        # One HDF5 per partition with tables_io (parallel, contention-free)\n",
    "        delayed_parts_to_write = ddf_out.to_delayed()\n",
    "        tables_io_kwargs = {}  # e.g., {\"complib\": \"blosc\", \"complevel\": 5} if supported\n",
    "\n",
    "        @delayed\n",
    "        def _write_h5_part_with_tables_io(pdf, out_path, kwargs):\n",
    "            pdf = pdf.reset_index(drop=True)\n",
    "            tables_io.write(pdf, out_path, **kwargs)\n",
    "            return out_path\n",
    "\n",
    "        write_tasks = []\n",
    "        for i, part in enumerate(delayed_parts_to_write):\n",
    "            out_path = os.path.join(data_dir, _file_name(i))\n",
    "            write_tasks.append(_write_h5_part_with_tables_io(part, out_path, tables_io_kwargs))\n",
    "\n",
    "        written_paths = dask.compute(*write_tasks)\n",
    "        print(f\"‚úÖ Wrote {len(written_paths)} HDF5 files to {data_dir} with tables_io\")\n",
    "\n",
    "print(\"‚úÖ Save complete (branching logic applied).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18b5a06-a869-4809-a1c8-82c0582574a0",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afd44ed-5dc2-4246-bc04-b4829b3d1753",
   "metadata": {},
   "source": [
    "## Reading the catalogs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b53781-07fa-4ca2-abd0-ab8033a3fe5c",
   "metadata": {},
   "source": [
    "Reading the input catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c573c03-f528-4c52-a233-ac0eb1dcc569",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DP_do_validation:\n",
    "    if DP_which_release == 'LSST_DP02':\n",
    "        ddf_input = dd.read_parquet(input_files, columns=input_user_selected_cols)\n",
    "\n",
    "        if DP_filter_by_boolean_column:\n",
    "            if DP_which_boolean_column not in ddf_input.columns:\n",
    "                raise ValueError(f\"Column '{DP_which_boolean_column}' not found in ddf_input.\")\n",
    "            ddf_input = ddf_input[ddf_input[DP_which_boolean_column] == DP_which_value_to_keep]\n",
    "\n",
    "        if DP_is_id_in_the_index:\n",
    "            ddf_input = ddf_input.reset_index()\n",
    "\n",
    "    elif DP_which_release == 'DES_DR2':\n",
    "        delayed_dfs = [delayed(read_fits_to_df_no_fix)(file, columns=input_user_selected_cols) for file in input_files]\n",
    "        ddf_input = dd.from_delayed(delayed_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2911300b-63c4-4ab3-ac53-279dab348cf9",
   "metadata": {},
   "source": [
    "Reading the output catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a64cae-0f3b-4bd9-8f6e-e91b419ac11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DP_do_validation:\n",
    "    output_folder = data_dir\n",
    "\n",
    "    if save_output_as == \"parquet\":\n",
    "        output_paths = os.path.join(output_folder, \"*.parquet\")\n",
    "        ddf_output = dd.read_parquet(output_paths)\n",
    "\n",
    "    elif save_output_as == \"csv\":\n",
    "        output_paths = os.path.join(output_folder, \"*.csv\")\n",
    "        ddf_output = dd.read_csv(output_paths, assume_missing=True)\n",
    "\n",
    "    elif save_output_as == \"hdf5\":\n",
    "        output_files = glob.glob(os.path.join(output_folder, \"*.h5\"))\n",
    "        delayed_dfs = [\n",
    "            delayed(tables_io.read)(file) for file in output_files\n",
    "        ]\n",
    "        ddf_output = dd.from_delayed(delayed_dfs)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Ouput format not supported for reading: {save_output_as}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c3a357-115d-4f8b-b12d-b328350fd489",
   "metadata": {},
   "source": [
    "Reading the template catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babb267f-dbb3-4337-87b6-9e4fbd9c7d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DP_do_validation:\n",
    "    if DP_compare_with_template:\n",
    "        template_files = glob.glob(os.path.join(template_path, template_pattern))\n",
    "\n",
    "        template_target_cols = [template_target_col.replace(\"BAND\", band) for band in template_bands_for_comparisson]\n",
    "        template_columns_to_read = [template_id_col] + template_target_cols\n",
    "\n",
    "        if DP_template_type == \"parquet\":\n",
    "            ddf_template = dd.read_parquet(template_files, columns=template_columns_to_read)\n",
    "\n",
    "        elif DP_template_type == \"fits\":\n",
    "            template_delayed_dfs = [\n",
    "                delayed(read_fits_to_df_no_fix)(file, columns=template_columns_to_read)\n",
    "                for file in template_files\n",
    "            ]\n",
    "            ddf_template = dd.from_delayed(template_delayed_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f0f89d-f397-46c4-b188-0f755ddc11b3",
   "metadata": {},
   "source": [
    "## Getting some basic informations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17d0c96-37a5-4527-a61d-661933ab977d",
   "metadata": {},
   "source": [
    "Printing the columns of the output dataframe and saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6219baea-90fb-4b86-a54f-ee0750bc34ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DP_do_validation:\n",
    "    input_selected_columns = list(ddf_input.columns)\n",
    "    print(\"üìå Selected columns in ddf_input:\")\n",
    "    print(input_selected_columns)\n",
    "    \n",
    "    output_columns = list(ddf_output.columns)\n",
    "    print(\"üìå Columns in ddf_output:\")\n",
    "    print(output_columns)\n",
    "    \n",
    "    if DP_save_the_data:\n",
    "        os.makedirs(logs_dir, exist_ok=True)\n",
    "        columns_path = os.path.join(logs_dir, \"input_output_columns.txt\")\n",
    "        with open(columns_path, \"w\") as f:\n",
    "            f.write(\"üìå Selected columns in ddf_input:\\n\")\n",
    "            for col in input_selected_columns:\n",
    "                f.write(col + \"\\n\")\n",
    "            f.write(\"\\nüìå Columns in ddf_output:\\n\")\n",
    "            for col in output_columns:\n",
    "                f.write(col + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89eca10-cbd7-4c37-928e-f3c6a7e49b67",
   "metadata": {},
   "source": [
    "Validating the sizes of input and output dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5768a252-bf2e-4952-96a2-c80cdf355b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DP_do_validation:\n",
    "    input_len = ddf_input.shape[0].compute()\n",
    "    print(f\"üì• Number of rows in input (filtered): {input_len}\")\n",
    "\n",
    "    # === Read output DataFrame ===\n",
    "    output_len = ddf_output.shape[0].compute()\n",
    "    print(f\"üì§ Number of rows in final output: {output_len}\")\n",
    "\n",
    "    # === Simple validation ===\n",
    "    if input_len == output_len:\n",
    "        print(\"‚úÖ Row count matches!\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Row count mismatch!\")\n",
    "    \n",
    "    match = input_len == output_len\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # Validation message\n",
    "    validation_text = f\"\"\"# ==== SIMPLE VALIDATION ====\n",
    "    Validation timestamp: {timestamp}\n",
    "\n",
    "    üì• Number of rows in input (filtered): {input_len}\n",
    "    üì§ Number of rows in final output: {output_len}\n",
    "\n",
    "    Result: {\"‚úÖ Row count matches!\" if match else \"‚ö†Ô∏è Row count mismatch!\"}\n",
    "    \"\"\"\n",
    "    \n",
    "    if DP_save_the_data:\n",
    "        validation_path = os.path.join(logs_dir, \"validation_info.txt\")\n",
    "        with open(validation_path, \"w\") as f:\n",
    "            f.write(validation_text)\n",
    "\n",
    "        print(f\"‚úÖ Validation result saved to: {validation_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cd8187-751e-46bb-a258-68c28916d249",
   "metadata": {},
   "source": [
    "Getting some IDs in both dataframes, printing and saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c042b2ab-cad4-45cb-aad7-257a583024be",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DP_do_validation:\n",
    "    selected_ids = ddf_input[id_col].head(10).tolist()\n",
    "\n",
    "    def filter_by_ids(df, col, values):\n",
    "        return df[df[col].isin(values)]\n",
    "\n",
    "    filtered_input = ddf_input.map_partitions(filter_by_ids, id_col, selected_ids, meta=ddf_input._meta)\n",
    "    filtered_output = ddf_output.map_partitions(filter_by_ids, id_col, selected_ids, meta=ddf_output._meta)\n",
    "\n",
    "    input_sample = filtered_input.compute().sort_values(id_col)\n",
    "    output_sample = filtered_output.compute().sort_values(id_col)\n",
    "\n",
    "    print(\"üì• Subset of ddf_input with selected IDs:\")\n",
    "    display(input_sample)\n",
    "\n",
    "    print(\"üì§ Subset of ddf_output with the same IDs:\")\n",
    "    display(output_sample)\n",
    "\n",
    "    if DP_save_the_data:\n",
    "        os.makedirs(logs_dir, exist_ok=True)\n",
    "\n",
    "        input_path = os.path.join(logs_dir, \"ddf_input_sample.csv\")\n",
    "        output_path = os.path.join(logs_dir, \"ddf_output_sample.csv\")\n",
    "\n",
    "        input_sample.to_csv(input_path, index=False)\n",
    "        output_sample.to_csv(output_path, index=False)\n",
    "\n",
    "        print(f\"‚úÖ CSV files saved to:\\n- {input_path}\\n- {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abf49e2-4865-4e77-9243-0282a358241f",
   "metadata": {},
   "source": [
    "Getting some IDs in both dataframes containing invalid values, printing and saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef6b43a-7dcb-4c1c-847d-25b259c3a387",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DP_do_validation and DP_replace_invalid_values:\n",
    "    print(\"\\nüîç Searching for 10 objects with invalid values in the output...\")\n",
    "\n",
    "    # Get the actual output column names (col/err)\n",
    "    col_pairs = []\n",
    "    for band in selected_bands:\n",
    "        if DP_pesonalized_which_band_case == 'lower_case':\n",
    "            band_formatted = band.lower()\n",
    "        elif DP_pesonalized_which_band_case == 'upper_case':\n",
    "            band_formatted = band.upper()\n",
    "        else:\n",
    "            band_formatted = band\n",
    "\n",
    "        col_out = DP_col_final_name_pattern.replace(\"BAND\", band_formatted)\n",
    "        err_out = DP_err_final_name_pattern.replace(\"BAND\", band_formatted)\n",
    "        col_pairs.append((col_out, err_out))\n",
    "\n",
    "    # Identify configured invalid replacement values\n",
    "    col_invalid_value = DP_col_value_to_replace\n",
    "    err_invalid_value = DP_err_value_to_replace\n",
    "\n",
    "    # Function to detect invalid rows in each partition\n",
    "    def find_invalid_rows(df):\n",
    "        mask = pd.Series(False, index=df.index)\n",
    "        for col, err in col_pairs:\n",
    "            if col in df.columns and err in df.columns:\n",
    "                if pd.isna(col_invalid_value):\n",
    "                    mask |= df[col].isna()\n",
    "                else:\n",
    "                    mask |= (df[col] == col_invalid_value)\n",
    "\n",
    "                if pd.isna(err_invalid_value):\n",
    "                    mask |= df[err].isna()\n",
    "                else:\n",
    "                    mask |= (df[err] == err_invalid_value)\n",
    "        return df[mask].head(10)\n",
    "\n",
    "    # Apply with Dask and collect results\n",
    "    invalid_rows_dd = ddf_output.map_partitions(find_invalid_rows, meta=ddf_output._meta)\n",
    "    invalid_rows = invalid_rows_dd.compute().drop_duplicates(subset=[id_col]).sort_values(id_col).head(10)\n",
    "\n",
    "    if len(invalid_rows) == 0:\n",
    "        print(\"‚úÖ No objects with invalid values found in the output.\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Found {len(invalid_rows)} objects with invalid values in the output:\")\n",
    "        display(invalid_rows)\n",
    "\n",
    "        # Fetch corresponding input objects\n",
    "        ids_with_invalid = invalid_rows[id_col].tolist()\n",
    "        filtered_input_invalid = ddf_input.map_partitions(filter_by_ids, id_col, ids_with_invalid, meta=ddf_input._meta)\n",
    "        input_invalid_sample = filtered_input_invalid.compute().sort_values(id_col)\n",
    "\n",
    "        print(\"üì• Matching entries in the input:\")\n",
    "        display(input_invalid_sample)\n",
    "\n",
    "        if DP_save_the_data:\n",
    "            os.makedirs(logs_dir, exist_ok=True)\n",
    "\n",
    "            invalid_input_path = os.path.join(logs_dir, \"invalid_input_sample.csv\")\n",
    "            invalid_output_path = os.path.join(logs_dir, \"invalid_output_sample.csv\")\n",
    "\n",
    "            input_invalid_sample.to_csv(invalid_input_path, index=False)\n",
    "            invalid_rows.to_csv(output_path, index=False)\n",
    "\n",
    "            print(f\"‚úÖ Invalid value samples saved to:\\n- {invalid_input_path}\\n- {invalid_output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d8f5e7-506e-4254-941a-22fad675f017",
   "metadata": {},
   "source": [
    "## Comparing with template catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f0056b-c89f-41cb-a492-e410b23e5a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DP_do_validation and DP_compare_with_template:\n",
    "    if len(selected_bands_for_comparisson) != len(template_bands_for_comparisson):\n",
    "        raise ValueError(\"selected_bands_for_comparisson and template_bands_for_comparisson must have the same length.\")\n",
    "\n",
    "    invalid_value = DP_col_value_to_replace if DP_replace_invalid_values else None\n",
    "\n",
    "    ddf_merged = ddf_output.merge(\n",
    "        ddf_template,\n",
    "        left_on=id_col,\n",
    "        right_on=template_id_col,\n",
    "        suffixes=(\"_output\", \"_template\")\n",
    "    )\n",
    "\n",
    "    comparison_lines = []\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    comparison_lines.append(\"# ==== GLOBAL COLUMN COMPARISON ====\")\n",
    "    comparison_lines.append(f\"Analysis timestamp: {timestamp}\")\n",
    "    comparison_lines.append(f\"Total band pairs compared: {len(selected_bands_for_comparisson)}\\n\")\n",
    "    comparison_lines.append(\"\\U0001F50D Comparing columns (using isclose with Dask):\\n\")\n",
    "\n",
    "    for band_out, band_template in zip(selected_bands_for_comparisson, template_bands_for_comparisson):\n",
    "        # Coluna no template\n",
    "        col_template = template_target_col.replace(\"BAND\", band_template)\n",
    "\n",
    "        # Coluna no output\n",
    "        if DP_pesonalized_which_band_case == 'lower_case':\n",
    "            band_fmt = band_out.lower()\n",
    "        elif DP_pesonalized_which_band_case == 'upper_case':\n",
    "            band_fmt = band_out.upper()\n",
    "        else:\n",
    "            band_fmt = band_out\n",
    "        col_out = DP_col_final_name_pattern.replace(\"BAND\", band_fmt)\n",
    "\n",
    "        if col_out == col_template:\n",
    "            col_out = f\"{col_out}_output\"\n",
    "            col_template = f\"{col_template}_template\"\n",
    "\n",
    "        if col_out in ddf_merged.columns and col_template in ddf_merged.columns:\n",
    "            if invalid_value is not None:\n",
    "                valid_mask = (\n",
    "                    (ddf_merged[col_out] != invalid_value) &\n",
    "                    (ddf_merged[col_template] != invalid_value) &\n",
    "                    ddf_merged[col_out].map_partitions(np.isfinite) &\n",
    "                    ddf_merged[col_template].map_partitions(np.isfinite)\n",
    "                )\n",
    "            else:\n",
    "                valid_mask = (\n",
    "                    ddf_merged[col_out].map_partitions(np.isfinite) &\n",
    "                    ddf_merged[col_template].map_partitions(np.isfinite)\n",
    "                )\n",
    "\n",
    "            ddf_valid = ddf_merged[valid_mask][[id_col, col_out, col_template]]\n",
    "            total = ddf_valid[id_col].count().compute()\n",
    "\n",
    "            is_diff = ddf_valid.map_partitions(\n",
    "                lambda df: pd.Series(\n",
    "                    ~np.isclose(df[col_out], df[col_template], atol=comparisson_precision, rtol=0),\n",
    "                    index=df.index\n",
    "                ),\n",
    "                meta=pd.Series(dtype=bool)\n",
    "            )\n",
    "            diff_count = is_diff.sum().compute()\n",
    "            percent_diff = (diff_count / total) * 100 if total > 0 else 0\n",
    "\n",
    "            msg = f\"\\U0001F4CF Band {band_out.upper()} vs {band_template.upper()}: {total} valid | {diff_count} different ({percent_diff:.5f}%)\"\n",
    "            print(msg)\n",
    "            comparison_lines.append(msg)\n",
    "\n",
    "            if diff_count > 0:\n",
    "                df_sample = ddf_valid.sample(frac=0.01, random_state=42).compute()\n",
    "                diff_mask = ~np.isclose(df_sample[col_out], df_sample[col_template], atol=comparisson_precision, rtol=0)\n",
    "                df_diffs = df_sample[diff_mask]\n",
    "\n",
    "                if not df_diffs.empty:\n",
    "                    preview_msg = \"\\n\\U0001F50E Sample differences:\"\n",
    "                    print(preview_msg)\n",
    "                    print(df_diffs.head())\n",
    "\n",
    "                    comparison_lines.append(preview_msg)\n",
    "                    comparison_lines.append(df_diffs.head().to_string(index=False))\n",
    "\n",
    "                    if DP_save_the_data:\n",
    "                        diff_path = os.path.join(logs_dir, f\"diff_sample_{band_out}_vs_{band_template}.csv\")\n",
    "                        df_diffs.to_csv(diff_path, index=False)\n",
    "        else:\n",
    "            msg = f\"\\u26A0\\uFE0F Band {band_out.upper()} vs {band_template.upper()}: missing columns -> {col_out} or {col_template}\"\n",
    "            print(msg)\n",
    "            comparison_lines.append(msg)\n",
    "\n",
    "    if DP_save_the_data:\n",
    "        os.makedirs(logs_dir, exist_ok=True)\n",
    "        comparison_path = os.path.join(logs_dir, \"global_comparison.txt\")\n",
    "        with open(comparison_path, \"w\") as f:\n",
    "            f.write(\"\\n\".join(comparison_lines))\n",
    "\n",
    "        print(f\"\\u2705 Global column comparison saved to: {comparison_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366371f5-8e73-44aa-92da-10e00d2ef4f4",
   "metadata": {},
   "source": [
    "# Closing the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577867e1-c51e-460c-8796-361fefebe7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()\n",
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_preparation",
   "language": "python",
   "name": "data_preparation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
